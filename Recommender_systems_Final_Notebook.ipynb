{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Movie Recommendation Systems\nThis notebook was created to solve the Kaggle EDSA Movie Recommendation Challenge.\n\nReccomendation sysytems are an integral part to any online user based service platform. In short an alogrithim is created that will reccomened you items (eg: movies) based on your history with past items.\n\nFor this challenge we are tasked to create a movie recommendation system, There were two paths that we could take.\nContent Based Filtering and Collaborative Filtering, We chose to try out both in which we will get further into this notebook\n\n### Problem statement:\nThis is a machine learning model that will predict how a user will rate a movie they have not yet viewed based on their historical preferences.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://imageproxy.themaven.net/https%3A%2F%2Fimages.saymedia-content.com%2F.image%2FMTY5ODE3MTkxNzY2NTY2MDg4%2Fnative-nerd-movie-list-of-45.jpg?\" width=55%>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Installations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install comet_ml","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from comet_ml import Experiment\n# experiment = Experiment(api_Key = vyyN9zt9jvAs1X1LBmL5WYbn1, project_name = 'unsupervised_predict', workspace = 'keane257')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install surprise","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Import  Libraries\nwe start by importing the various libraries we will need","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# General\nimport numpy as np\nimport pandas as pd\nimport scipy as sp # <-- The sister of Numpy, used in our code for numerical efficientcy. \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n\n# Machine Learning\n\n # scipy\nfrom scipy.spatial.distance import cdist\nfrom scipy import stats\nfrom scipy import sparse\nfrom scipy.sparse import csr_matrix\n\n # sklearn\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import silhouette_score\nfrom sklearn import preprocessing\n\n # surprise\nimport surprise \nfrom surprise import SVD\nfrom surprise.model_selection import cross_validate\nfrom surprise import Dataset\nfrom surprise import accuracy\nfrom surprise import Reader\nfrom surprise import NormalPredictor\nfrom surprise import KNNBasic\nfrom surprise import KNNWithMeans\nfrom surprise import KNNWithZScore\nfrom surprise import KNNBaseline\nfrom surprise import BaselineOnly\nfrom surprise import SVDpp\nfrom surprise import NMF\nfrom surprise import SlopeOne\nfrom surprise import CoClustering\nfrom surprise.accuracy import rmse\nfrom surprise.model_selection import train_test_split\nfrom surprise import KNNBaseline,SlopeOne,CoClustering\n\n# Entity featurization and similarity computation\nfrom sklearn.metrics.pairwise import cosine_similarity \nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Libraries used during sorting procedures.\nimport operator # <-- Convienient item retrieval during iteration \nimport heapq # <-- Efficient sorting of large lists\n\n# Imported for our sanity\nimport warnings\nwarnings.filterwarnings('ignore')\nimport math\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Converting the Data into DataFrames and exploring metrics of the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tags=pd.read_csv(r'/kaggle/input/edsa-recommender-system-predict/tags.csv')\nTrain=pd.read_csv(r'/kaggle/input/edsa-recommender-system-predict/train.csv')\ngenome_scores=pd.read_csv(r'/kaggle/input/edsa-recommender-system-predict/genome_scores.csv')\ngenome_tags=pd.read_csv(r'/kaggle/input/edsa-recommender-system-predict/genome_tags.csv')\nMovie=pd.read_csv(r'/kaggle/input/edsa-recommender-system-predict/movies.csv')\nimdb=pd.read_csv(r'/kaggle/input/edsa-recommender-system-predict/imdb_data.csv')\ntest=pd.read_csv(r'/kaggle/input/edsa-recommender-system-predict/test.csv')\nlinks=pd.read_csv(r'/kaggle/input/edsa-recommender-system-predict/links.csv')\nsubmission=pd.read_csv(r'/kaggle/input/edsa-recommender-system-predict/sample_submission.csv')\n\ntrain = Train.copy()\nmovies = Movie.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# d={'train':Train.shape,'movies':movies.shape,'links':links.shape,\n#    'idmb':imdb.shape,'genome_tags':genome_tags.shape,'genome_scores':genome_scores.shape}\n# data =pd.DataFrame(data=d,index=['rows','columns'])\n# data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the number of records and the total number of movies\nprint('The dataset contains: ', len(train), ' ratings of ', len(movies), ' movies.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Exploratory Data Analyses (EDA)\n\nA visual summary of the dataframe to identify patterns and trends and communicate the insights of the data through visual representation\nWe first merge the train and movie dataframe to get a more detailed information of the movieid thats on train dataframe to find the title and genre and then we rename the merged dataframes as df","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Movie Director Analyses\nUsing imdb_data to analyse the directors and the influence they have on the rating ,budget aswell as analysis of active directors based on the movies they produce with user ratings\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.merge(train,movies,on='movieId')\ndf2=pd.merge(imdb,df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nexploring the new dataframe to get statical analyses , to see any null values,data type and the structure of the dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'The SHape of DataFrame: {df2.shape}')\nprint('\\nInfo about the DataFrame:')\nprint(df2.info())\nprint(f'\\nMAx rating of movie: {df2.rating.max()}')\nprint(f'\\nMin rating of movie: {df2.rating.min()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have merged,we start by creating a copy to remove duplicates to get more accurate results on the number of movies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df3=df2.copy()\ndf3.drop_duplicates('movieId',inplace=True)\ndf3.director.value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the top 10 directors who produced the most movies,Director Luc Besson has the highest movies produced as compared to other directors and the director with the least movies is Peter Farrelly. now that we know the producers with the most movie we will check the ratings with regars to the movies and the users have rated their movies.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_directors = df3.groupby('director').agg({'rating': 'mean', 'movieId': 'count'})\ntop_5_rating_directors = rating_directors.rename(columns={'movieId': 'Movies Count'}).sort_values('rating', ascending=False)[:5]\ntop_5_rating_directors","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nThe director with the top movie rating is Stephen Zotnowski, even though he produced least amount of movies he still has the the highest user movie ratings","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = top_5_rating_directors['rating'].plot.bar();\nax.set_ylabel('IMDB Rating')\nax.set_title('Average IMDB Rating of Top 5 Directors by IMDB Rating', y=1.02)\nax.set_xticklabels(top_5_rating_directors.index, rotation = 45);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nThese are the top 5 directors with the highest ratings even though they did not produce a lot movies their movies produced are rated as 5.0 . which means more movies produced by this directors are enjoyed by the viewers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"most_active_directors = df2['director'].value_counts()[df2['director'].value_counts() >= 5].head(10)\nmost_active_directors","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The director with the most movie rated by the users is Quentin Tarantino this implies that his movies have the highest viewers compared to other movies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"active_director_movies = df2[df2.director.isin(most_active_directors.index.tolist())]\nactive_director_movies.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"active_director_stats = active_director_movies.groupby('director')['budget', 'rating'].mean()\nactive_director_stats.sort_values(by='rating',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the top 10 directors who have movies with the highest number of ratings Jonathan Nolan has the highest rating. this concludes that his movies are being watched and enjoyed by the users","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Ratings Analyses\n**In this section we will first analyse the movies,users and their rating trends based on average votes and movies watched.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def human(num, units = 'M'):\n    units = units.lower()\n    num = float(num)\n    if units == 'k':\n        return str(num/10**3) + \" K\"\n    elif units == 'm':\n        return str(num/10**6) + \" M\"\n    elif units == 'b':\n        return str(num/10**9) +  \" B\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nplt.title('Distribution of ratings over Training dataset', fontsize=15)\nsns.countplot(df2.rating)\nax.set_yticklabels([human(item, 'M') for item in ax.get_yticks()])\nax.set_ylabel('No. of Ratings(Millions)')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this bar graph we can see that very view movies are rated as 0.5 and most average rated movies are 4.0 with ratings over 2.5 million. now that we have seen the highest ratings we will evaluate the title of movies with the highest ratings","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"highest_ratings = pd.DataFrame(df2.groupby('title')['rating'].mean().sort_values(ascending=False).head(5))\nhighest_ratings","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Final Recourse is rated as the top movie with the highest rating, now that we have seen the ratings we need to find out how many users have voted for these movies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"highest_ratings=pd.DataFrame(df2.groupby('title')['rating'].count().sort_values(ascending=False)).head()\nhighest_ratings.reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Movie Shawshank Redemption has the highest number of ratings this means that this movie has been watched a lot by the viewers how ever even though its on the top ten most rated it does not have the highest rating .we will analyse this further by evaluating the number of ratings.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total data \")\nprint(\"-\"*50)\nprint(\"\\nTotal no of ratings :\",Train.shape[0])\nprint(\"Total No of Users   :\", len(np.unique(Train.userId)))\nprint(\"Total No of movies  :\", len(np.unique(Train.movieId)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above we have 1000038 number of ratings with 162541 users and 48213 number of movies which means that movie Shawshank Redemption has 33.8% of the ratings this could be its the most watched movies hence it has the highest number of ratings. next step we look at the distibution of number of ratings","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"highest_num_rate= pd.DataFrame(df.groupby('title')['rating'].mean())\nhighest_num_rate['num of ratings'] = pd.DataFrame(df.groupby('title')['rating'].count().sort_values(ascending=False))\nplt.figure(figsize=(10,4))\nhighest_num_rate['num of ratings'].hist(bins=100);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nhighest_num_rate['rating'].hist(bins=100);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(x='rating',y='num of ratings',data=highest_num_rate,alpha=0.5);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Date and Time Analyses","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"movies2=pd.merge(Movie,Train,on='movieId')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean1(text):\n    text =text.replace('(','')\n    text =text.replace('|','')\n    text =text.replace(')','')\n    text =text.replace('-','')\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies1=movies2.copy()\nmovies1['year'] = movies1['title'].str.split('(').str[1]\nmovies1['year']=movies1['year'].str.replace(\")\",\"\")\nmovies1['year'] = pd.to_numeric(movies1.year, errors='coerce').fillna(0, downcast='infer')\nmovies1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies1 =movies1.replace(0, np.nan)\nmovies1 =movies1.replace(6, np.nan)\nmovies1 = movies1.dropna(how='all', axis=0)\nmovies1.dropna(inplace=True)\nmovies1.isnull().sum()\nmovies1['year'].astype(int)\nmovies1['year']=pd.to_datetime(movies1.year, format='%Y')\nmovies1['year'] = pd.DatetimeIndex(movies1['year']).year\nmovies1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_rating = movies1.groupby('year')['rating'].mean()\nmovies_rating.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = movies_rating.plot.line();\nax.set_title('Average Movie Rating per Year', y=1.04)\nax.set_ylabel('Rating');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the realease movies by years movies that were produced in 1940 till 2020 have the highest averahe rating as compard to 1900 as the year incereases so as the number of ratings however there is a slight decrease from 1980.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_year = movies1.groupby('year')['movieId'].count()\n\n# Plot the movies count per year\nax = movies_year.plot.line();\nax.set_title('Movies count per Year', y=1.04)\nax.set_ylabel('Movies Count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Number of ratings per month","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"timestamps = df.copy()\n\nmovies1['timestamp'] = pd.to_datetime(movies1['timestamp'],unit = 'ms')\nmovies1['year'] = movies1['timestamp'].dt.year\n\nmovies1['month'] = movies1['timestamp'].dt.month\n\nmovies1['day'] = movies1['timestamp'].dt.day\nmovies1['day of week'] = movies1['timestamp'].dt.day_name()\nmovies1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies_year = movies1.groupby('day')['rating'].count()\nmovies_year","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Number of ratings per day","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=plt.figaspect(.45))\nsns.boxplot(y='rating', x='day of week', data=movies1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nsns.countplot(x='day of week', data=movies1, ax=ax)\nplt.title('No of ratings on each day...')\nplt.ylabel('Total no of ratings')\nplt.xlabel('')\nax.set_yticklabels([human(item, 'M') for item in ax.get_yticks()])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## User Analyses","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"New_df = Train.join(Movie.set_index('movieId'), on='movieId')\nNew_df.head()\nUser = New_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from plotly.offline import init_notebook_mode, plot, iplot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\n\ndata = User['rating'].value_counts().sort_index(ascending=False)\ntrace = go.Bar(x = data.index,\n               text = ['{:.1f} %'.format(val) for val in (data.values / df.shape[0] * 100)],\n               textposition = 'auto',\n               textfont = dict(color = '#000000'),\n               y = data.values,\n               )\n# Create layout\nlayout = dict(title = 'Distribution Of {} Movie-ratings'.format(df.shape[0]),\n              xaxis = dict(title = 'Rating'),\n              yaxis = dict(title = 'Count'))\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of ratings per movie\ndata = User.groupby('movieId')['rating'].count().clip(upper=50)\n\n# Create trace\ntrace = go.Histogram(x = data.values,\n                     name = 'Ratings',\n                     xbins = dict(start = 0,\n                                  end = 50,\n                                  size = 2))\n# Create layout\nlayout = go.Layout(title = 'Distribution Of Number of Ratings Per Movie (Clipped at 100)',\n                   xaxis = dict(title = 'Number of Ratings Per Movie'),\n                   yaxis = dict(title = 'Count'),\n                   bargap = 0.2)\n\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = User.groupby('userId')['rating'].count().clip(upper=50)\n\n# Create trace\ntrace = go.Histogram(x = data.values,\n                     name = 'Ratings',\n                     xbins = dict(start = 0,\n                                  end = 50,\n                                  size = 2))\n# Create layout\nlayout = go.Layout(title = 'Distribution Of Number of Ratings Per User (Clipped at 50)',\n                   xaxis = dict(title = 'Ratings Per User'),\n                   yaxis = dict(title = 'Count'),\n                   bargap = 0.2)\n\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Genre Analyses","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining new DataFrame, to not affect original\ngenre_rate=movies2.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to clean text\ndef clean1(text):\n    text =text.replace('(','')\n    text =text.replace('|','')\n    text =text.replace(')','')\n    text =text.replace('-','')\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"genre_rate['genres']=genre_rate['genres'].str.replace('|',\",\")\ngenre_rate.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"genre_rate['Genre_count'] =genre_rate.genres.str.split(',').str.len()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"genre_rate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"genre_count_stats = genre_rate.groupby('Genre_count')['rating'].mean()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"genre_count_stats = genre_rate.groupby('Genre_count')['rating'].mean()\nax1= genre_count_stats.plot.bar(subplots=True);\nplt.title('# average ratings based on number of genres')\nplt.xlabel('ratings')\nplt.ylabel('No of ratings');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining\ngenre_3_df = genre_rate[genre_rate.Genre_count == 7]\ngenre_3_df = genre_3_df.groupby('genres')['Genre_count', 'rating'].mean()\n\n# Plotting\nax = genre_3_df.rating.sort_values(ascending=False).head(5).plot.barh();\nax.set_xlabel('Rating');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Clustering\nCluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis in this section we will do a cluster analyses using KMeans and hierachy to evaluate the behavour of users and combine users based on their similarities.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating and scaling X variable\nX = User[:1000].iloc[:, [1, 2]].values\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating copies of the data\ntrain2=train.copy()\nmovie2=movies.copy()\n\n# Modifying the data\ntrain2.drop(columns='timestamp',axis=1,inplace=True)\ndf2 = pd.merge(train2 , movie2 , how='outer', on='movieId')\nfrom itertools import chain\ndf2.dropna(inplace=True)\n\n# function to return list from series of comma-separated strings\ndef chainer(s):\n    return list(chain.from_iterable(s.str.split('|')))\n\n# calculate lengths of splits\nlens = df2['genres'].str.split('|').map(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df2= pd.DataFrame({'userId': np.repeat(df2['userId'], lens),\n                    'movieId': np.repeat(df2['movieId'], lens),\n                    'rating': np.repeat(df2['rating'], lens),\n                    'genres': chainer(df2['genres'])})                                                \nnew_df2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_genre_ratings(new_df2, genres, column_names):\n    genre_ratings = pd.DataFrame()\n    for genre in genres:        \n        genre_movies = new_df2[new_df2['genres'].str.contains(genre) ]\n        avg_genre_votes_per_user = new_df2[new_df2['movieId'].isin(genre_movies['movieId'])].loc[:, ['userId', 'rating']].groupby(['userId'])['rating'].mean().round(2)\n        \n        genre_ratings = pd.concat([genre_ratings, avg_genre_votes_per_user], axis=1)\n        \n    genre_ratings.columns = column_names\n    return genre_ratings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Elbow Method","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating elbow plot\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1, 11), wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use this visualization in order to obtain the optimal clusters for the data, we look for the bend point in the data, as we try to optimize the computational power while still having enough data to accuratly build a model from the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clustering with Kmeans\nkmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 42,max_iter=300)\ny_kmeans = kmeans.fit_predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\nplt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\nplt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')\n# plot the centroids\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],s=250, marker='*',c='red', edgecolor='black',label='centroids')\nplt.legend(scatterpoints=1)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hierarchy cluster with Dendrogram","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.cluster.hierarchy as sch\ndendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\nplt.title('Dendrogram')\nplt.xlabel('user')\nplt.ylabel('no of rating')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is another way to visualize the data to obtain the optimal amount of clusters similar to the Elbow Method , we use this to pick the clusters that will maximize our efficientcy in the modelling process.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\nhc = AgglomerativeClustering(n_clusters = 4, affinity = 'euclidean', linkage = 'ward')\ny_hc = hc.fit_predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\nplt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\nplt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\n\nplt.title('Clusters of user')\nplt.xlabel('ratings')\nplt.ylabel('movies)')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Recommender system (Modeling)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Content Based Filtering\nThis type of filter does not involve other users if not ourselves. Based on what we like, the algorithm will simply pick items with similar content to recommend us, based on our personal history with the items that we chose.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src= \" https://image.ibb.co/f6mDXU/conten.png \"  width=45%>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"movie =pd.merge(movies,tags,on='movieId')\nmovie.drop(columns='timestamp',inplace=True)\nmovie","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie['genres']=movie['genres'].str.replace('|','')\nmovie.fillna('',inplace=True)\nmovie['metadata']=movie[['genres','tag']].apply(lambda x:''.join(x),axis=1)\nmovie","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings=pd.merge(train,movies,on='movieId')\nratings.drop(columns=['timestamp','genres'],inplace=True)\nratings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Entity featurization and similarity computation\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Libraries used during sorting procedures.\nimport operator # <-- Convienient item retrieval during iteration \nimport heapq # <-- Efficient sorting of large lists\n\n# Imported for our sanity\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new = User[:1000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf = TfidfVectorizer(analyzer='word', ngram_range=(1,2),\n                     min_df=0, stop_words='english')\n\n# Produce a feature matrix, where each row corresponds to a book,\n# with TF-IDF features as columns \ntf_authTags_matrix = tf.fit_transform(df_new['title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"similar = cosine_similarity(tf_authTags_matrix,\n                                        tf_authTags_matrix)\nprint (similar.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"similar[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"util_matrix = df_new.pivot_table(index=['userId'],\n                                       columns=['title'],\n                                       values='rating')\nutil_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a neat version of the utility matrix to assist with plotting book titles \ndf_new['title'] = df_new['title'].apply(lambda x: x[:20])\nutil_matrix_neat = df_new.pivot_table(index=['userId'],\n                                            columns=['title'],\n                                            values='rating')\n\nfig, ax = plt.subplots(figsize=(15,5))\n# We select only the first 100 users for ease of computation and visualisation. \n# You can play around with this value to see more of the utility matrix. \n_ = sns.heatmap(util_matrix_neat[:100], annot=False, ax=ax).set_title('Movie')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titles = movie['title']\nindices = pd.Series(movie.index, index=movie['title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings_f=train.groupby('userId').filter(lambda x:len(x)<=2)\nmovie_list_rating =ratings_f.movieId.unique().tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings_f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie =movie[movie.movieId.isin(movie_list_rating)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf = TfidfVectorizer(analyzer = 'word', ngram_range = (1,2),\n                     min_df = 0, stop_words = 'english')\n\n# Produce a feature matrix, where each row corresponds to a book,\n# with TF-IDF features as columns \ntf_authTags_matrix = tf.fit_transform(movie['metadata']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cosine_sim_authTags = cosine_similarity(tf_authTags_matrix, \n                                        tf_authTags_matrix)\nprint (cosine_sim_authTags.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def content_generate_top_N_recommendations(title, N=10):\n    # Convert the string book title to a numeric index for our \n    # similarity matrix\n    b_idx = indices[title]\n    # Extract all similarity values computed with the reference book title\n    sim_scores = list(enumerate(cosine_sim_authTags[b_idx]))\n    # Sort the values, keeping a copy of the original index of each value\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    # Select the top-N values for recommendation\n    sim_scores = sim_scores[1:N]\n    # Collect indexes \n    book_indices = [i[0] for i in sim_scores]\n    # Convert the indexes back into titles \n    return titles.iloc[book_indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# content_generate_top_N_recommendations('Casino (1995)', N=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def content_generate_rating_estimate(title, user, rating_data, k=20, threshold=0.0):\n    # Convert the book title to a numeric index for our \n    # similarity matrix\n    b_idx = indices[title]\n    neighbors = [] # <-- Stores our collection of similarity values \n     \n    # Gather the similarity ratings between each book the user has rated\n    # and the reference book \n    for index, row in rating_data[rating_data['user_id']==user].iterrows():\n        sim = cosine_sim_authTags[b_idx-1, indices[row['title']]-1]\n        neighbors.append((sim, row['rating']))\n    # Select the top-N values from our collection\n    k_neighbors = heapq.nlargest(k, neighbors, key=lambda t: t[0])\n\n    # Compute the weighted average using similarity scoress and \n    # user item ratings. \n    simTotal, weightedSum = 0, 0\n    for (simScore, rating) in k_neighbors:\n        # Ensure that similarity ratings are above a given threshold\n        if (simScore > threshold):\n            simTotal += simScore\n            weightedSum += simScore * rating\n    try:\n        predictedRating = weightedSum / simTotal\n    except ZeroDivisionError:\n        # Cold-start problem - No ratings given by user. \n        # We use the average rating for the reference item as a proxy in this case \n        predictedRating = np.mean(rating_data[rating_data['title']==book_title]['rating'])\n    return predictedRating","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comet","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# params = {'model' = 'tf', \n#          'analyzer' = 'word',\n#          'ngram_range' = (1,2),\n#          'min_df' = 0}\n# experiment.log_params(params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Collabartive Based Filtering (App)\nThis type of filter involves other users. Based on what we like, the algorithm will search for similar users who like the same items and reccomend items to us that they have liked.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](http://ars.els-cdn.com/content/image/1-s2.0-S0167739X17308890-gr1.jpg )","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modifying the Data\n\ntr = train.copy()\ntr = tr.drop('timestamp', axis=1) # Removing the unwanted column\n\nmerged = pd.merge(tr,movies,on='movieId') # Merging the dataframes\nmerge = merged.copy()\nmerge = merged.drop('genres', axis=1) # Dropping the unwanted columns\n\nmerge = merge[:500000] # Slicing the data so that there is less computational power required\nm = merge.pivot_table(index=['userId'],columns=['title'],values='rating') # pivoting the table into a matrix\n\nuserRatings = m.dropna(thresh=10, axis=1).fillna(0,axis=1)\ncorrMatrix = userRatings.corr(method='pearson') # using pearson correlation\ncorrMatrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating Function to extract similar movies\n\ndef get_similar(movie_name,rating=5):\n    similar_ratings = corrMatrix[movie_name]*(rating-2.5)\n    similar_ratings = similar_ratings.sort_values(ascending=False)\n    return similar_ratings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a list for 3 Movies, for the recommendations to be based on\n\nfav_movies = [(\"1984 (1956)\",5),(\"Terminator, The (1984)\",5),(\"Back to the Future Part III (1990)\",5)]\n\nsimilar_movies = pd.DataFrame() # creating empty DataFrame\nfor movie,rating in fav_movies:\n    similar_movies = similar_movies.append(get_similar(movie,rating),ignore_index = True) # Filling up the DataFrame with similar movies\n\nsimilar_movies.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calling the function\nrecc_movies = similar_movies.sum().sort_values(ascending=False).head(40)[3:13]\n\nprint('Top recommendations based on your movie choices:\\n')\ncount = 1\n\nfor key, value in dict(recc_movies).items():\n    print(str(count) + '. ' + str(key))\n    count += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from surprise import Reader, Dataset\nreader = Reader()\ndf = train[:50000]\ndata = Dataset.load_from_df(df[['userId', 'movieId', 'rating']], reader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from surprise.model_selection import train_test_split\n\ntrainset, testset = train_test_split(data, test_size=0.10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from surprise import SVD, accuracy\nalgo = SVD()\nalgo.fit(trainset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = algo.test(testset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from surprise import accuracy\naccuracy.rmse(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre=[]\nfor _,row in test.iterrows():\n    x_unseen=algo.predict(row['userId'],row['movieId'])\n    pred=x_unseen[3]\n    pre.append(pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Kaggle Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"id = []\nfor i in test[['userId', 'movieId']].values:\n    id.append(str(i[0]) + '_' + str(i[1]))\n# new_train['id'] = new_train[]\ntest['id'] = id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_test=test['id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Final_table={'id':final_test,'rating':np.round(pre,1)}\nsubmission=pd.DataFrame(data=Final_table)\nsubmission=submission[['id','rating']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('New20.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comet","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# experiment.end()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}